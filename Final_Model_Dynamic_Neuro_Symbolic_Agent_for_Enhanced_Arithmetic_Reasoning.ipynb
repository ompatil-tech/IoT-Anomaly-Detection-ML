{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN9z+2Hahj2hDi0uzfEI5HN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "368cb3ce7127471da511b8cdb8a41c1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c4bed2f1d9141cda932e9d69d01c092",
              "IPY_MODEL_f7723832af2c451a9948bc36b7e386ac",
              "IPY_MODEL_b17e6b3e38da43b8b29231f3f190e28c"
            ],
            "layout": "IPY_MODEL_e499027ac13548e195da4a88c3090628"
          }
        },
        "3c4bed2f1d9141cda932e9d69d01c092": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b35cd0a93dc4a86953039a1ca2252fe",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d7be02bede6442d191acded268df21e7",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "f7723832af2c451a9948bc36b7e386ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36d8debc0d424025b6bb29f130800584",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_774ca19091a8457fbc651fd26748fb0f",
            "value": 2
          }
        },
        "b17e6b3e38da43b8b29231f3f190e28c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2230ebcfc1134fac817e328b4d44431b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7cd0a7ab754d442e990a5a2ebf5790b6",
            "value": "‚Äá2/2‚Äá[00:56&lt;00:00,‚Äá25.93s/it]"
          }
        },
        "e499027ac13548e195da4a88c3090628": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b35cd0a93dc4a86953039a1ca2252fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7be02bede6442d191acded268df21e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36d8debc0d424025b6bb29f130800584": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "774ca19091a8457fbc651fd26748fb0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2230ebcfc1134fac817e328b4d44431b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cd0a7ab754d442e990a5a2ebf5790b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ompatil-tech/IoT-Anomaly-Detection-ML/blob/main/Final_Model_Dynamic_Neuro_Symbolic_Agent_for_Enhanced_Arithmetic_Reasoning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# DIRECT DOWNLOAD BY ID\n",
        "# ==========================================\n",
        "\n",
        "# 1. INSTALL LIBRARIES\n",
        "import subprocess\n",
        "print(\"üõ†Ô∏è Installing libraries...\")\n",
        "subprocess.run(\"pip install -q transformers peft accelerate bitsandbytes gdown\", shell=True)\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "import re\n",
        "import ast\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Callable, Dict\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. DOWNLOAD FILES DIRECTLY (Bypassing Drive Mounting)\n",
        "# ---------------------------------------------------------\n",
        "MODEL_DIR = \"my_fine_tuned_model\"\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "    os.makedirs(MODEL_DIR)\n",
        "\n",
        "print(f\"‚¨áÔ∏è Downloading model files into '{MODEL_DIR}'...\")\n",
        "\n",
        "# File 1: adapter_config.json\n",
        "# ID: 1WXM4hHkIjMG46dkEA7hzl5mqRva2k3YF\n",
        "subprocess.run(f\"gdown 1WXM4hHkIjMG46dkEA7hzl5mqRva2k3YF -O {MODEL_DIR}/adapter_config.json\", shell=True)\n",
        "\n",
        "# File 2: adapter_model.safetensors\n",
        "# ID: 1HJ3SINyzfO8A64BlIK6Pav3uCuNQt-Q4\n",
        "subprocess.run(f\"gdown 1HJ3SINyzfO8A64BlIK6Pav3uCuNQt-Q4 -O {MODEL_DIR}/adapter_model.safetensors\", shell=True)\n",
        "\n",
        "# Verify download\n",
        "if os.path.exists(f\"{MODEL_DIR}/adapter_config.json\") and os.path.exists(f\"{MODEL_DIR}/adapter_model.safetensors\"):\n",
        "    print(\"‚úÖ Files downloaded successfully!\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"‚ùå Download failed. Google might be blocking the download due to high traffic. Try again in 1 minute.\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. DEFINE AGENT & TOOLS\n",
        "# ---------------------------------------------------------\n",
        "def extract_json_object(text: str) -> Dict[str, Any]:\n",
        "    s = text.strip()\n",
        "    s = re.sub(r\"^```[a-zA-Z]*\", \"\", s)\n",
        "    s = re.sub(r\"```$\", \"\", s).strip()\n",
        "    match_list = re.search(r'\\[\\s*\\{.*?\\}\\s*\\]', s, re.DOTALL)\n",
        "    if match_list:\n",
        "        try:\n",
        "            data = json.loads(match_list.group(0))\n",
        "            if isinstance(data, list) and len(data) > 0: return data[0]\n",
        "        except: pass\n",
        "    match_obj = re.search(r\"\\{.*\\}\", s, re.DOTALL)\n",
        "    if match_obj:\n",
        "        try: return json.loads(match_obj.group(0))\n",
        "        except: pass\n",
        "    raise ValueError(f\"No JSON found in: {text[:20]}...\")\n",
        "\n",
        "@dataclass\n",
        "class Tool:\n",
        "    name: str\n",
        "    description: str\n",
        "    func: Callable\n",
        "\n",
        "def calculator_func(args):\n",
        "    expr = str(args.get(\"expression\", \"\"))\n",
        "    clean_expr = re.sub(r'(\\d),(\\d)', r'\\1\\2', expr)\n",
        "    try:\n",
        "        allowed = set(\"0123456789+-*/(). \")\n",
        "        if not set(clean_expr).issubset(allowed): return {\"error\": \"Unsafe chars\"}\n",
        "        return {\"result\": eval(clean_expr)}\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}\n",
        "\n",
        "calc_tool = Tool(\"calculator\", \"Math evaluator\", calculator_func)\n",
        "\n",
        "class ToolAugmentedLlama:\n",
        "    def __init__(self, model, tokenizer, tools):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tools = {t.name: t for t in tools}\n",
        "\n",
        "    def generate(self, user_input):\n",
        "        # ROBOT PROMPT\n",
        "        system_prompt = \"\"\"[SYSTEM: CODE MODE]\n",
        "You are a JSON generator. You DO NOT chat.\n",
        "Output ONLY a JSON object for the tool.\n",
        "FORMAT: {\"tool\": \"calculator\", \"args\": {\"expression\": \"100+50\"}}\n",
        "\"\"\"\n",
        "        prompt = f\"[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{user_input} [/INST]\"\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        try:\n",
        "            tool_call = extract_json_object(response)\n",
        "            if tool_call[\"tool\"] in self.tools:\n",
        "                result = self.tools[tool_call[\"tool\"]].func(tool_call[\"args\"])\n",
        "                return {\"final_answer\": f\"Calculated: {result}\", \"tool_calls\": [tool_call]}\n",
        "        except: pass\n",
        "        return {\"final_answer\": response, \"tool_calls\": []}\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. LOAD MODEL (Your Fine-Tuned Version)\n",
        "# ---------------------------------------------------------\n",
        "print(f\"üîÑ Loading Model from {MODEL_DIR}...\")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"NousResearch/Llama-2-7b-chat-hf\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "# Load Adapter from the specific download folder\n",
        "model = PeftModel.from_pretrained(base_model, MODEL_DIR)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")\n",
        "print(\"‚úÖ Model Loaded!\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5. RUN TEST\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "368cb3ce7127471da511b8cdb8a41c1b",
            "3c4bed2f1d9141cda932e9d69d01c092",
            "f7723832af2c451a9948bc36b7e386ac",
            "b17e6b3e38da43b8b29231f3f190e28c",
            "e499027ac13548e195da4a88c3090628",
            "5b35cd0a93dc4a86953039a1ca2252fe",
            "d7be02bede6442d191acded268df21e7",
            "36d8debc0d424025b6bb29f130800584",
            "774ca19091a8457fbc651fd26748fb0f",
            "2230ebcfc1134fac817e328b4d44431b",
            "7cd0a7ab754d442e990a5a2ebf5790b6"
          ]
        },
        "id": "8Qna-oo0y9Z8",
        "outputId": "e2ad1479-ef18-4c94-e06e-ee834513ba68"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üõ†Ô∏è Installing libraries...\n",
            "‚¨áÔ∏è Downloading model files into 'my_fine_tuned_model'...\n",
            "‚úÖ Files downloaded successfully!\n",
            "üîÑ Loading Model from my_fine_tuned_model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "368cb3ce7127471da511b8cdb8a41c1b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model Loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# FINAL ROBUST AGENT: IMPROVED LOGIC EXAMPLES\n",
        "# ==========================================\n",
        "import json\n",
        "import re\n",
        "import torch\n",
        "\n",
        "class ToolAugmentedLlama:\n",
        "    def __init__(self, model, tokenizer, tools):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.tools = {t.name: t for t in tools}\n",
        "\n",
        "    def generate(self, user_input):\n",
        "        # 1. IMPROVED PROMPT (TEACHING THE LOGIC)\n",
        "        # We give examples that show DEPENDENCIES (C depends on B).\n",
        "        prompt = f\"\"\"You are a calculator. Extract numbers and calculate sum.\n",
        "\n",
        "Example 1 (Simple):\n",
        "Input: The Apple is 5. The Pear is 10. The Banana is 20. Calculate sum.\n",
        "Output: {{\"tool\": \"calculator\", \"args\": {{\"expression\": \"5 + 10 + 20\"}}}}\n",
        "\n",
        "Example 2 (Complex Dependencies):\n",
        "Input: Box A is 100. Box B is double Box A. Box C is 50 less than Box B.\n",
        "Output: {{\"tool\": \"calculator\", \"args\": {{\"expression\": \"100 + (100*2) + ((100*2)-50)\"}}}}\n",
        "\n",
        "Example 3 (Mixed):\n",
        "Input: The Shirt is 20. Pants are double the Shirt. Hat is 5 more than Pants.\n",
        "Output: {{\"tool\": \"calculator\", \"args\": {{\"expression\": \"20 + (20*2) + ((20*2)+5)\"}}}}\n",
        "\n",
        "Task:\n",
        "Input: {user_input}\n",
        "Output: {{\"tool\": \"calculator\", \"args\": {{\"expression\": \"\n",
        "\"\"\"\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        with self.model.disable_adapter():\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(**inputs, max_new_tokens=128, do_sample=False)\n",
        "\n",
        "        full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # 2. PARSING\n",
        "        try:\n",
        "            # Grab everything after the prompt ends\n",
        "            raw_math = full_text.split('expression\": \"')[-1]\n",
        "\n",
        "            # Trim whitespace and newlines\n",
        "            clean_math = raw_math.strip()\n",
        "\n",
        "            # Stop at the first quote or newline\n",
        "            if '\"' in clean_math:\n",
        "                clean_math = clean_math.split('\"')[0]\n",
        "            else:\n",
        "                clean_math = clean_math.split('\\n')[0]\n",
        "\n",
        "            # Reconstruct valid JSON\n",
        "            json_reconstructed = '{\"tool\": \"calculator\", \"args\": {\"expression\": \"' + clean_math + '\"}}'\n",
        "\n",
        "            tool_call = json.loads(json_reconstructed)\n",
        "            if \"tool\" in tool_call and tool_call[\"tool\"] in self.tools:\n",
        "                result = self.tools[tool_call[\"tool\"]].func(tool_call[\"args\"])\n",
        "                return {\n",
        "                    \"final_answer\": f\"Calculated Result: {result}\",\n",
        "                    \"tool_calls\": [tool_call]\n",
        "                }\n",
        "        except Exception as e:\n",
        "             return {\n",
        "                 \"final_answer\": f\"Fix failed: {e}\",\n",
        "                 \"raw_output\": full_text,\n",
        "                 \"tool_calls\": []\n",
        "             }\n",
        "\n",
        "        return {\"final_answer\": \"Failed\", \"raw_output\": full_text, \"tool_calls\": []}\n",
        "\n",
        "# Re-Initialize\n",
        "agent = ToolAugmentedLlama(model, tokenizer, [calc_tool])\n",
        "\n",
        "print(\"ü§ñ RUNNING COMPLEX LOGIC TEST...\")\n",
        "test_prompt = \"\"\"The Pen costs 5 dollars. The Notebook costs triple that amount. The Highlighter costs 2 dollars. Calculate the total cost. \"\"\"\n",
        "\n",
        "result = agent.generate(test_prompt)\n",
        "\n",
        "print(\"\\n=== FINAL RESULT ===\")\n",
        "print(result[\"final_answer\"])\n",
        "print(\"\\n=== RAW TOOL CALL ===\")\n",
        "if \"tool_calls\" in result and result[\"tool_calls\"]:\n",
        "    print(json.dumps(result[\"tool_calls\"], indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WlaK3Zr9WlQ",
        "outputId": "30bb2fed-0566-445e-c61d-f1ea54b46f7f"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ RUNNING COMPLEX LOGIC TEST...\n",
            "\n",
            "=== FINAL RESULT ===\n",
            "Calculated Result: {'result': 22}\n",
            "\n",
            "=== RAW TOOL CALL ===\n",
            "[\n",
            "  {\n",
            "    \"tool\": \"calculator\",\n",
            "    \"args\": {\n",
            "      \"expression\": \"5 + (3*5) + 2\"\n",
            "    }\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 6. LAUNCH INTERACTIVE UI (Gradio)\n",
        "# ==========================================\n",
        "import gradio as gr\n",
        "import json\n",
        "import subprocess\n",
        "\n",
        "# 1. Install Gradio (if not already installed)\n",
        "try:\n",
        "    import gradio\n",
        "except ImportError:\n",
        "    print(\"üõ†Ô∏è Installing Gradio UI...\")\n",
        "    subprocess.run(\"pip install -q gradio\", shell=True)\n",
        "    import gradio as gr\n",
        "\n",
        "# 2. Define the Wrapper Function\n",
        "def solve_puzzle(user_text):\n",
        "    \"\"\"\n",
        "    This function connects the Web UI inputs to your Agent's generate method.\n",
        "    \"\"\"\n",
        "    if not user_text.strip():\n",
        "        return \"Please enter a puzzle.\", \"{}\"\n",
        "\n",
        "    # Run the Agent (The agent object 'agent' must be loaded from your previous cell)\n",
        "    try:\n",
        "        response = agent.generate(user_text)\n",
        "\n",
        "        # Format the output for the user\n",
        "        answer = response.get(\"final_answer\", \"No final answer generated.\")\n",
        "\n",
        "        # Format the tool logs for debugging/grading\n",
        "        tools_used = response.get(\"tool_calls\", [])\n",
        "        tool_log = json.dumps(tools_used, indent=2) if tools_used else \"No tools used (check final answer for raw model output).\"\n",
        "\n",
        "        return answer, tool_log\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error processing request: {str(e)}\", \"{}\"\n",
        "\n",
        "# 3. Create the Web Interface\n",
        "demo = gr.Interface(\n",
        "    fn=solve_puzzle,\n",
        "    inputs=gr.Textbox(\n",
        "        lines=5,\n",
        "        placeholder=\"Type your logic puzzle here...\\nExample: Tank A has 40 fish. Tank B has triple that amount. Tank C has 20 fish. Calculate the total number of fish.\",\n",
        "        label=\"Your Logic Puzzle\"\n",
        "    ),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Agent Solution (Calculated Result)\"),\n",
        "        gr.Code(label=\"Tool Logic (For Grading)\", language=\"json\")\n",
        "    ],\n",
        "    title=\"Neuro-Symbolic Agent Tester\",\n",
        "    description=\"Enter a word problem. The Agent uses Llama-2-7B to extract the logic and a Python calculator to find the numerical result. \",\n",
        "    theme=\"soft\"\n",
        ")\n",
        "\n",
        "# 4. Launch!\n",
        "print(\"üöÄ Launching UI... Click the public link below to access the interface!\")\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "Idrh5MAUL71_",
        "outputId": "7545655c-56c8-40a0-a94d-103396e04a63"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Launching UI... Click the public link below to access the interface!\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://305ef4c52ec25952a4.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://305ef4c52ec25952a4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://305ef4c52ec25952a4.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    }
  ]
}